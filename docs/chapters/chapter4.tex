\chapter{An\'alisis de resultados}

\label{Chapter4}

A lo largo del presente cap\'itulo, se analizar\'an los resultados obtenidos a partir de los experimentos planteados.
Se detallar\'an las m\'etricas obtenidas, se visualizarán los espacios latentes $\mathbf{z}$ generados por la parte
convolucional de las redes y se analizarán los errores que surgen de aplicar los modelos a los telegramas.

\section{An\'alisis de m\'etricas}

Los experimentos realizados arrojaron los resultados mostrados en el cuadro \ref{tab:metricas-experimentos}. Las
m\'etricas referidas a la capacidad de clasificaci\'on (Accuracy, $F_1$) son evaluadas sobre la partici\'on de test del
dataset de origen donde se conocen las etiquetas a ciencia cierta ($MNIST$). Por otro lado, las m\'etricas de
adaptaci\'on ($MMD$, Dist. $\mathcal{A}$) son evaluadas sobre los espacios latentes que los modelos generaron para las
particiones de test de ambos datasets. El mejor modelo es el que posea mejor capacidad de clasificaci\'on y de
adaptaci\'on.

\begin{table}[H]
    \centering
    \begin{tabular}{cc|rrrr}
        \toprule
                                     &        & Acc.                                & $F_1$                               & $MMD$                               & Dist. $\mathcal{A}$                 \\
        AD                           & Modelo &                                     &                                     &                                     &                                     \\
        \midrule
        \multirow[c]{2}{*}{-}        & ResNet & {\footnotesize (2)} 0.9885          & {\footnotesize (2)} 0.9883          & 0.0632                              & 1.9306                              \\
                                     & LeNet  & 0.9811                              & 0.9810                              & 0.0469                              & 1.9515                              \\\hline
        \multirow[c]{2}{*}{DANN}     & ResNet & \textbf{{\footnotesize (1)} 0.9890} & \textbf{{\footnotesize (1)} 0.9890} & 0.1379                              & 1.9776                              \\
                                     & LeNet  & 0.9822                              & 0.9821                              & {\footnotesize (3)} 0.0090          & 1.6774                              \\\hline
        \multirow[c]{2}{*}{ADDA}     & ResNet & 0.9476                              & 0.9485                              & 0.0165                              & 1.8495                              \\
                                     & LeNet  & 0.9191                              & 0.9184                              & 0.0399                              & 1.8495                              \\\hline
        \multirow[c]{2}{*}{DANN+BSP} & ResNet & 0.9780                              & 0.9777                              & 0.0409                              & 1.7888                              \\
                                     & LeNet  & 0.9859                              & 0.9857                              & {\footnotesize (2)} 0.0051          & {\footnotesize (3)} 1.6369          \\\hline
        \multirow[c]{2}{*}{MDD}      & ResNet & {\footnotesize (3)} 0.9864          & {\footnotesize (3)} 0.9863          & 0.0615                              & 1.8987                              \\
                                     & LeNet  & 0.9856                              & 0.9854                              & 0.0399                              & 1.7468                              \\\hline
        \multirow[c]{2}{*}{AFN}      & ResNet & 0.9829                              & 0.9827                              & \textbf{{\footnotesize (1)} 0.0040} & \textbf{{\footnotesize (1)} 1.0886} \\
                                     & LeNet  & 0.9862                              & 0.9859                              & 0.0117                              & {\footnotesize (2)} 1.5747          \\

        \bottomrule
    \end{tabular}
    \caption{M\'etricas de los experimentos realizados. Entre par\'entesis se encuentra la posici\'on que ocupa dentro del top 3 de la columna.}
    \label{tab:metricas-experimentos}
\end{table}

Como era de esperarse, los modelos que fueron entrenados sin adaptaci\'on de dominio son los que mejores m\'etricas de
clasificaci\'on poseen. No obstante, sus valores de adaptaci\'on son los peores provocando que no puedan generalizar a
$TDS$.

El modelo que mejor combinaci\'on de clasificaci\'on y adaptaci\'on es la ResNet utilizando AFN. Cabe destacar que los
modelos LeNet entrenados con DANN y AFN obtuvieron buenas m\'etricas en general, lo que deja en evidencia que {\it no
        siempre un modelo m\'as complejo es mejor}.

Es posible aplicar los modelos sobre todos los telegramas y calcular el $IoU$ promedio por telegrama con el c\'alculo
descripto en el cap\'itulo \ref{Chapter3} y la cantidad de aciertos promedio por telegrama utilizando como etiquetas lo
transcripto en el centro de c\'omputo suponiendo que existen pocos errores en ellos.

\begin{table}[H]
    \centering
    \begin{tabular}{cc|rrr}
        \toprule
                                     &        & $IoU$ prom.     & \# aciertos prom. & \% aciertos prom. \\
        AD                           & Modelo &                 &                   &                   \\
        \midrule
        \multirow[c]{2}{*}{-}        & ResNet & 0.4494          & 4                 & 22\%              \\
                                     & LeNet  & 0.4715          & 6                 & 33\%              \\\hline
        \multirow[c]{2}{*}{DANN}     & ResNet & 0.6941          & 12                & 67\%              \\
                                     & LeNet  & 0.7024          & 12                & 67\%              \\\hline
        \multirow[c]{2}{*}{ADDA}     & ResNet & 0.6763          & 11                & 61\%              \\
                                     & LeNet  & 0.6406          & 10                & 56\%              \\\hline
        \multirow[c]{2}{*}{DANN+BSP} & LeNet  & 0.6695          & 11                & 61\%              \\
                                     & ResNet & 0.6515          & 11                & 61\%              \\\hline
        \multirow[c]{2}{*}{MDD}      & ResNet & 0.5451          & 8                 & 44\%              \\
                                     & LeNet  & 0.5801          & 9                 & 50\%              \\\hline
        \multirow[c]{2}{*}{AFN}      & ResNet & \textbf{0.7486} & \textbf{13}       & \textbf{72\%}     \\
                                     & LeNet  & 0.6493          & 11                & 61\%              \\
        \bottomrule
    \end{tabular}
    \caption{IoU promedio y cantidad promedio de aciertos al aplicar los modelos a cada telegrama.}
    \label{tab:iou-cant-aciertos-en-telegramas}
\end{table}

El cuadro \ref{tab:iou-cant-aciertos-en-telegramas} confirma la elecci\'on del mejor modelo realizada anteriormente.
Independientemente de qu\'e t\'ecnica de adaptaci\'on se utilice, todas presentan mejores porcentajes de aciertos
promedio que los modelos que fueron entrenados \'unicamente con $MNIST$.

TODO: Actualizar para agregar DANN + BSP. TODO: buscar una mejor forma de mostrar los histogramas para que se vea las
distintas distribuciones de aciertos dependiendo de cada modelo y AD.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{chapter4/dist-aciertos.png}
    \caption{Distribuci\'on de aciertos de cantidad de votos por cada par t\'ecnica AD y modelo.}
    \label{fig:distribucion-aciertos}
\end{figure}

\section{Análisis de los espacios latentes}

La capacidad de adaptación de dominio es una medida que se utiliza para evaluar el rendimiento de un modelo de
aprendizaje automático en un dominio de prueba diferente al dominio en el que se entrenó. Uno de los factores clave que
influyen en la capacidad de adaptación de dominio es la calidad de los espacios latentes que genera el modelo durante
la etapa de entrenamiento.

El espacio latente en el contexto de las redes convolucionales se refiere a una representación de los datos de entrada
que se ha aprendido durante el entrenamiento del modelo. Durante dicho proceso, la red aprende a extraer
características relevantes de las imágenes de entrada a través de la etapa convolucional.

El espacio latente $\mathbf{z}$ se genera después de la etapa convolucional y antes de la etapa densa de la red. En
este espacio latente, cada dimensión representa una característica específica de la imagen de entrada. Por ejemplo, en
el caso de una red convolucional entrenada para clasificar imágenes de gatos y perros, una dimensión podría representar
la forma de las orejas y otra dimensión podría representar el color de la nariz.

Para evaluar la capacidad de adaptación de dominio de un modelo, se pueden comparar las distribuciones de los espacios
latentes generados para diferentes conjuntos de datos. En general, se espera que los espacios latentes generados para
diferentes conjuntos de datos sean similares, ya que esto indica que el modelo es capaz de generalizar.

Si los espacios latentes son iguales o similares para diferentes conjuntos de datos, la etapa densa de la red que se
encarga de la clasificación puede realizar predicciones precisas para el dominio de destino. Por otro lado, si los
espacios latentes son muy diferentes para diferentes conjuntos de datos, esto puede indicar que el modelo no es capaz
de adaptarse a nuevos conjuntos de datos y, por lo tanto, su capacidad de generalización es limitada.

Una forma común de visualizarlos es mediante una técnica de reducción de dimensionalidad, como Uniform Manifold
Approximation and Projection \parencite{mcinnes2018umap}. UMAP es una técnica no lineal que se utiliza para visualizar estructuras de datos complejas en
un espacio de menor dimensión.

Al visualizar los espacios latentes utilizando UMAP, se pueden observar patrones y relaciones en los datos que de otra
manera serían difíciles de detectar. Al comparar las distribuciones de los espacios latentes generados por un modelo
para diferentes conjuntos de datos, se pueden identificar las similitudes y diferencias entre los dominios, lo que
puede ser útil para evaluar la capacidad de adaptación de dominio del modelo.

En las siguientes subsecciones se analizarán las representaciones UMAP de los espacios latentes obtenidos por cada
modelo entrenado.

\subsection{LeNet}

La figura \ref{fig:umaps-lenet} a continuación contiene las representaciones UMAP obtenidas de los espacios latentes
generados por todos los modelos LeNet.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-lenet-so.png}
        \caption{LeNet entrenada con MNIST.}
        \label{fig:umap-lenet-so}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-lenet-dann.png}
        \caption{LeNet entrenada mediante DANN.}
        \label{fig:umap-lenet-dann}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-lenet-adda.png}
        \caption{LeNet entrenada mediante ADDA.}
        \label{fig:umap-lenet-adda}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-lenet-bsp.png}
        \caption{LeNet entrenada mediante DANN+BSP.}
        \label{fig:umap-lenet-bsp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-lenet-mdd.png}
        \caption{LeNet entrenada mediante MDD.}
        \label{fig:umap-lenet-mdd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-lenet-afn.png}
        \caption{LeNet entrenada mediante AFN.}
        \label{fig:umap-lenet-afn}
    \end{subfigure}

    \caption{Representaciones UMAP de los espacios latentes de los modelos LeNet. Los puntos azules representan observaciones de MNIST y los naranjas de TDS.}
    \label{fig:umaps-lenet}
\end{figure}

Se pueden observar los siguientes resultados:

\begin{itemize}
    \item Entrenar el modelo solo con MNIST no permite una buena generalización para TDS, como se muestra en la figura
          \ref{fig:umap-lenet-so}.
    \item Con el entrenamiento mediante ADDA o MDD se comienzan a generar agrupaciones en MNIST, pero no logran generarse de la
          misma manera en TDS, como se puede apreciar en las figuras \ref{fig:umap-lenet-adda} y \ref{fig:umap-lenet-mdd}.
    \item Al entrenar el modelo mediante DANN, se generan agrupaciones similares para ambos conjuntos de datos, como se muestra
          en la figura \ref{fig:umap-lenet-dann}.
    \item Finalmente, al utilizar la técnica de entrenamiento DANN con penalización BSP y AFN, se generan agrupaciones similares
          y más densas para ambos conjuntos de datos, tal como se muestra en las figuras \ref{fig:umap-lenet-bsp} y
          \ref{fig:umap-lenet-afn}.
\end{itemize}

En conclusión, se puede afirmar que la técnica de adaptación de dominio utilizada tiene un gran impacto en el
rendimiento de un modelo LeNet debido a su estructura simple. Entre las diferentes técnicas evaluadas, se puede
observar que DANN+BSP y AFN son las que proporcionan una mejor capacidad de adaptación al modelo LeNet. En general,
estos resultados resaltan la importancia de seleccionar cuidadosamente la técnica de adaptación de dominio adecuada
para un modelo dado a fin de lograr un rendimiento óptimo en diferentes escenarios de aplicación.

\subsection{ResNet}

La figura \ref{fig:umaps-resnet} a continuación contiene las representaciones UMAP obtenidas de los espacios latentes
generados por todos los modelos ResNet.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-resnet-so.png}
        \caption{ResNet entrenada con MNIST.}
        \label{fig:umap-resnet-so}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-resnet-dann.png}
        \caption{ResNet entrenada mediante DANN.}
        \label{fig:umap-resnet-dann}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-resnet-adda.png}
        \caption{ResNet entrenada mediante ADDA.}
        \label{fig:umap-resnet-adda}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-resnet-bsp.png}
        \caption{ResNet entrenada mediante DANN+BSP.}
        \label{fig:umap-resnet-bsp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-resnet-mdd.png}
        \caption{ResNet entrenada mediante MDD.}
        \label{fig:umap-resnet-mdd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.40\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/umap-resnet-afn.png}
        \caption{ResNet entrenada mediante AFN.}
        \label{fig:umap-resnet-afn}
    \end{subfigure}

    \caption{Representaciones UMAP de los espacios latentes de los modelos ResNet. Los puntos azules representan observaciones de TDS y los naranjas de MNIST.}
    \label{fig:umaps-renet}
\end{figure}

Al analizar las representaciones UMAP de los espacios latentes obtenidos por los modelos ResNet, se pueden observar los
siguientes resultados:

\begin{itemize}
    \item En contraste con los modelos LeNet, la ResNet es más eficaz en la extracción de características {\it out of the box},
          lo que le permite generalizar mejor sin necesidad de técnicas de adaptación de dominio (figura
          \ref{fig:umap-resnet-so}).
    \item La aplicación de técnicas de adaptación de dominio como DANN, DANN con penalización BSP y ADDA mejora la capacidad de
          generalización, pero no de forma significativa en comparación con las mejoras obtenidas con LeNet (figuras
          \ref{fig:umap-resnet-dann}, \ref{fig:umap-resnet-bsp} y \ref{fig:umap-resnet-adda}).
    \item El uso de Margin Disparity Discrepancy parece disminuir la capacidad de generalización (figura
          \ref{fig:umap-resnet-mdd}).
    \item La aplicación de Adaptive Feature Norm potencia la generalización de la ResNet, obteniendo representaciones de los
          espacios latentes prácticamente idénticas para ambos conjuntos de datos (figura \ref{fig:umap-resnet-afn}).
\end{itemize}

En general, estas representaciones permiten visualizar los resultados obtenidos a partir de las métricas de adaptación
descritas en el cuadro \ref{tab:metricas-experimentos} de la sección anterior.

\section{\'Analisis de errores}

Los errores de predicci\'on de los modelos pueden reflejarse en las distribuci\'on de la m\'etrica $IoU$ para cada uno
de ellos.

\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.43\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/hist-iou-sin-da.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.43\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/hist-iou-dann.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.43\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/hist-iou-adda.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.43\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/hist-iou-bsp.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.43\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/hist-iou-mdd.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.43\textwidth}
        \includegraphics[height=1\textwidth]{chapter4/hist-iou-afn.png}
    \end{subfigure}

    \caption{Histogramas de la m\'etrica $IoU$ promedio por telegrama por cada t\'ecnica AD y modelo.}
    \label{fig:histogramas-ious}
\end{figure}

Resulta interesante mencionar existen telegramas que son m\'as dif\'iciles de analizar que otros. Esto puede
evidenciarse en los histogramas de la figura \ref{fig:histogramas-ious} donde se pueden observar un conjunto de
observaciones que contienen valores entre [0, 0.2] en todos los experimentos realizados. Luego de analizar cada uno de
estos casos, se detectaron las siguientes situaciones:

\begin{itemize}
    \item Telegramas cargados de forma err\'onea: ver ejemplo en el anexo \ref{anexo:telegrama-erroneo}.
    \item Telegramas correctos pero por alguna cuesti\'on la l\'ogica de extracci\'on de d\'igitos no funciona correctamente: ver
          ejemplo en el anexo \ref{anexo:telegrama-numeros-juntos}.
    \item Telegramas donde existen otros caracteres distintos a n\'umeros: al ser un cuadro de texto libre sin formato, los jefes
          de mesa pueden escribir lo que deseen. Ver ejemplo en el anexo \ref{anexo:telegrama-erroneo-caracteres-especiales}
          donde se representa el $0$ a la izquierda con $X$.
    \item Telegramas de mesas donde la mayor cantidad de votos se las lleva un \'unico partido y completan los votos a la
          izquierda con $0$: al agregar los ceros a la izquierda, aumenta la probabilidad de que el modelo se equivoque con esos
          ceros que no aportan al n\'umero final. Ver ejemplo en el anexo \ref{anexo:telegrama-erroneo-muchos-ceros}.
\end{itemize}

En los primeros dos puntos se describen problemas problemas que fueron detectados en el proceso de ETL del cap\'itulo
\ref{Chapter3}. Estandarizar los telegramas agregando un casillero por cada d\'igito junto a mejorar el proceso de
extracci\'on de los mismos, supondr\'a una mejora considerable en las capacidades predictivas de los modelos.

El tercer punto presenta un problema dentro de la adaptaci\'on de dominio. La misma supone que, si bien los datasets de
origen y destino son diferentes pero representan lo mismo, debe existir la misma cantidad de clases entre origen y
destino. Al agregar uno o varios caracteres adicionales en $TDS$ (como es el ejemplo donde representaban el $0$ con una
$X$), se est\'a incumpliendo este supuesto.

El cuarto punto aumenta la probabilidad de error en los modelos debido a que el $0$ a la izquierda no aporta
significado alguno al n\'umero de la cantidad de votos que se desea predecir.

Estandarizar la carga de los telegramas por parte de los jefes de mesa mediante alguna capacitaci\'on permitir\'ia
reducir los errores de los puntos tres y cuatro en elecciones futuras.

