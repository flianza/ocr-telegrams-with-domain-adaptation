\chapter{Introducci\'on}

\label{Chapter1}

\section{Elecciones en Argentina}

En Argentina se celebran elecciones cada 2 a\~{n}os a excepci\'on de las presidenciales que se realizan cada 4
a\~{n}os. En Argentina se realizan tres tipos de elecciones:

\begin{itemize}
    \item Elecciones nacionales, para elegir a las autoridades federales del pa√≠s: el Poder Ejecutivo, constituido por el
          Presidente y el vicepresidente y el Congreso Nacional, formado por Senadores y Diputados.
    \item Elecciones provinciales y de la Ciudad de Buenos Aires o locales, para elegir a las autoridades de cada provincia: los
          poderes ejecutivos de las provincias y sus legislaturas.
    \item Elecciones municipales, regidas por las leyes y procedimientos de cada provincia.
\end{itemize}

Si bien emitir el sufragio es diferente en cada una de ellas, generalmente consta de ingresar a un cuarto oscuro,
elegir el candidato que se desea y depositar el voto en una urna. Al finalizar la jornada, las autoridades de mesas
recuentan los votos y llenan una planilla a mano alzada donde se resume la cantidad de votos obtenidos por cada
candidato o partido pol\'itico. Dicha planilla es escaneada y enviada a traves de un telegrama correo argentino al
centro de c\'omputo para su procesamiento. Una vez all\'i, se contabilizan en un sistema inform\'atico una por una. Por
la metodolog\'ia de contabilizac\'ion, idealmente lo escrito a mano en el telegrama y lo computado en el sistema es lo
mismo. Sin embargo, como esta tarea es realizada por personas, es plausible pensar que pueden haber errores en dicho
proceso.

(TODO: ver de agregar un diagrama aca del proceso)
(TODO: ver de agregar una imagen de un telegrama en el apendice o anexo)

A su vez, durante la jornada electoral existe una ansiedad generalizada para ir sabiendo los resultados parciales y
finales de la misma, por lo que se debe contratar a una gran cantidad personas destinadas al centro de c\'omputo. En
las elecciones legislativas del 2021 se gastaron unos \$17.000 millones de pesos de los cuales \$4.000 millones de
pesos fueron destinados a sueldos para el personal\footnote{Fuente:
    \href{https://www.cronista.com/economia-politica/Elecciones-legislativas-2021-cuanto-mas-se-gastara-por-el-coronavirus-segun-el-Presupuesto-20201004-0006.html}{El
        cronista}}. Mejorar el proceso manual de contabilizaci\'on de los telegramas supondr\'a un ahorro considerable en el
presupuesto de las elecciones, agilizar\'a la obtenci\'on de los resultados y aportar\'a transparencia al proceso en
general.

(TODO: buscar alguna noticia de ver si algun pais ya digitaliza automaticamente o khe)

\section{Clasificaci\'on de d\'igitos (TODO: ???)}

La clasificaci\'on de d\'igitos escritos a mano lleva resuelto hace un tiempo con una performance \'optima.
\cite{lecun1998gradient} propone una arquitectura de red neuronal con m\'ultiples capas y clasifica correctamente el
dataset {\it MNIST} con ella. Se podr\'ia proponer un modelo similar para esto.

La detecci\'on de d\'igitos en los telegramas de elecciones en Argentina podr\'ia llevarse a cabo mediante un modelo
entrenado en {\it datasets} de d\'igitos p\'ublicos como el {\it MNIST} \cite{lecun1998gradient}. Como no existe una
\'unica forma de escribir, el modelo estar\'a sesgado a reconocer d\'igitos escritos de forma similar a los que se
encontraban en el {\it dataset} de entrenamiento. No ser\'a capaz de generalizar lo aprendido en un dominio distinto.

En trabajos anteriores, se aplican distorsiones al conjunto de entrenamiento para aumentar la cantidad de datos de
entrenamiento y de esta forma el modelo pueda generalizar y aplicarse a los telegramas de elecciones de la Ciudad de
Buenos Aires \cite{lamagna2016lectura}. En el presente trabajo se utilizar\'an t\'ecnicas referidas al {\it transfer
        learning}, espec\'ificamente de {\it domain adaptation} para resolver el problema.

\section{Deep Learning}

Cuando se habla de {\it Deep learning}, se hace referencia a una serie de algoritmos de {\it machine learning} que son
capaces de utilizar m\'ultiples capas de procesamiento de forma que puedan aprender representaciones de los datos con
diferentes niveles de abstracci\'on \parencite{lecun2015deep}. Estos algoritmos, denominados redes neuronales profundas (o DNNs por sus siglas en ingl\'es),
poseen la capacidad de encontrar variables que expliquen la naturaleza del comportamiento de los datos.

Los modelos obtenidos a partir del {\it deep learning} han demostrado tener gran capacidad de aprendizaje para todo
tipo de problemas, como ser {\it computer vision} \parencite{szeliski2010computer, redmon2016yolo}, procesamiento del lenguaje natural \parencite{devlin2018bert}, reconocimiento del habla \parencite{hannun2014deep}, juegos \parencite{silver2016mastering}, generaci\'on de im\'agenes a partir de descripciones \parencite{ramesh2022dalle2}, entre otros.

Aunque la utilidad de estos modelos se encuentra demostrada y d\'ia a d\'ia so utilizados en diferentes \'ambitos de la
vida, presentan un gran problema: la enorme cantidad de datos que requieren para su entrenamiento. La mayor\'ia de los
modelos que mejores m\'etricas de performance presentan, necesitan millones de datos en sus datasets de entrenamiento.
Esto implica que, para que los mismos sean de utilidad, es de suma importancia los procesos de recolecci\'on y
etiquetado de los datos. La eficacia de los modelos queda altamente relacionada con la calidad de los datos que se
posean o se logren conseguir. Particularme, el etiquetado de los datos es una tarea costosa, ineficiente y hasta a
veces resulta inviable de realizar \parencite{reis2022data}.

Una posible soluci\'on a este problema consiste en emular la capacidad que tienen los humanos de adquirir conocimiento
relevante en un \'area y aplicarlo en otra similar \parencite{thrun1998learning}. Es decir, poder {\it transferir} lo aprendido. En el caso del {\it deep learning}, lo que se
busca es que la red aprenda representaciones lo suficientemente generales para que despues sean utilizados en el
entrenamiento de una tarea similar. Esto busca acortar los tiempos de entrenamiento, mejorar las predicciones y hacer
los modelos m\'as robustos.
